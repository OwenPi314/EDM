EDM (Emotion Detecting Mask)
The EDM has two major components, an autorecorder and an emotion classifier.
The autorecorder will be installed on the mask, whenever the user
speaks, the recorder will recorder the user speaking and store it on
an SD card.
Then, the stored speech is fed into the emotion classifier which then 
determines what emotions the user is experiencing when saying a particular
sentence. This data, after analysis, can be used for a better
awareness and mindfulness of the emotions that have been experienced
throughout the day so that the user can better reflect and manage
themselves.

Known Issues: 
  - the audio quality from the auto-recorder is significantly
    worse than the audio clips from the datasets, this could mean
    that the model generated from the datasets won't perform well
     - Possible solution:
       - Look into noise cancelling algorithms
       - Buy better components for the recorder [x because I'm poor]
       - Buy a recorder instead of building one [x because I'm poor]
  - Format of the input vs. dataset: 1. Since the input only contains
    one sentence, I need to find a way so that the recorded input
    contain one sentence each.
     - Possible solution:
       - Apply another machine learning model to determine the start
         and end of a sentence before sending it into the emotion
         classifier
----------------------------------------------------------------------------

Emotion Classifier

The emotion classifier aims to classify audio clips of human speech
into emotions.

Dataset(s) used:
  - "The Ryerson Audio-Visual Database of Emotional 
     Speech and Song (RAVDESS)" by Livingstone & Russo is 
     licensed under CC BY-NA-SC 4.0.

Features implemented:
Emotion intensity, speaker gender, Fourier Transform of entire
audio clips, Spectrogram, [NEXT UP] MFCC
  [NOTE] FFT of entire audio clips have slightly worse performance than
         spectrograms.

Models implemented:
K-Nearest Neighbors, [NEXT UP] Generative Gaussian Model.

[NOTE] Using FFT features + PCA + KNN and splitting the data based on
       gender showed some improvement, so it could be a good idea
       to keep the data seperate from now on.
[NOTE] For features with high dimensionality, PCA is applied to improve
       performance.
----------------------------------------------------------------------------

AutoRecorder

The auto recorder starts a audio recording when you start talking
and ends recording when you stop talking.

Components:
  - Arduino Uno
  - SD card              (to store the recordings)
  - SD card adapter
  - Analog audio sensor  (for recording)
  - Digital audio sensor (for checking whether there is sound)

How it works:
  - It constantly checks for input to the digital sensor, once sound
    is detected, recording starts using the analog sensor
  - When no sound has been detected for a whole second, the recording
    stops and waits for the person to start talking again

  - The recorded audio files are stored on an SD card
  - The files are named in the format "naudio.wav" where n is the
    number of recordings before this one

Usage:
  - Start to use it throughout the day
  - When finished, use the files in the SD card for analysis
  - When done with the analysis, remove all files on the SD card

Possible improvements:
1. There could be better settings for how long to wait before deciding
   that there is no sound
   - Perhaps a better setting can be obtained though machine learning?
   - Perhaps more testing?
2. The recording misses the beginning of the recording by a little bit
